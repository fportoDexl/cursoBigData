{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K-means-MR.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fportoDexl/cursoBigData/blob/main/K_means_MR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Versão spark\n",
        "  Neste exemplo consideramos que geramos 1000 pontos randomicos, no intervalo\n",
        "  [1,5000] e queremos clusterizar em k grupos."
      ],
      "metadata": {
        "id": "JglQBkEgxWFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iniciamos instalando o pyspark"
      ],
      "metadata": {
        "id": "ItQBlXjrxwqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiGEOlx23cED",
        "outputId": "9c4ac84a-394f-4bc5-a2c0-ffa213f91cba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 53.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=7f4bfcc564fb801a2376e03ca2440ebf0ce6292f392e270be865fb8779237eb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importamos pyspark - API python para Spark\n",
        "##            radom - para geração pesudo-aleatório de inteiros"
      ],
      "metadata": {
        "id": "Thz_F268x52m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark as ps\n",
        "import random"
      ],
      "metadata": {
        "id": "Rk7rU1Ar3xa_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importamos o SparkContext para definição do espaço RDD\n",
        "Deve existir apenas um SparkContext por job spark"
      ],
      "metadata": {
        "id": "N4DI7mN0yN0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.context import SparkContext\n",
        "sc= SparkContext(appName=\"K-means\", master=\"local[4]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "YHMm1Dw7ACHr",
        "outputId": "db87ef5d-4b5f-4215-8c73-c2d4b13995cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-aa69463f2934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"K-means\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"local[4]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    353\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 355\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=K-means, master=local[4]) created by __init__ at <ipython-input-3-aa69463f2934>:2 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iniciamos um acumulador - Fica armazenado no Driver e pode ser acumulado \n",
        "## a partir dos nós trabalhadores"
      ],
      "metadata": {
        "id": "fA6nXqqjyebn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acum=sc.accumulator(0)"
      ],
      "metadata": {
        "id": "M1guqjQ6wqst"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criamos uma classe para iniciar as duas listas:\n",
        "\n",
        "*   centroids - conjunto de centroids dos clustes. Inicialmente vazia\n",
        "*   points - lista de pontos a clusterizar\n",
        "\n",
        "\n",
        "  - "
      ],
      "metadata": {
        "id": "C_X8mtlzyzPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class K_means_exemplo_MR:\n",
        "  def __init__(self, k):\n",
        "    self.centroids=list(random.sample(range(1,1000),k))\n",
        "    self.points=sc.parallelize(random.sample(range(1,5000),1000),2)\n",
        "  \n"
      ],
      "metadata": {
        "id": "aJ8tmRlCchSp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função findCluster\n",
        "  input= p-> ponto sendo avaliado; centroids-> lista de centroids\n",
        "  Objetivo:\n",
        "    c=varrer a lista e encontrar o centroid mais proximo ao ponto \"p\""
      ],
      "metadata": {
        "id": "HtJf8ZHhzMMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findCluster(p,centroids):\n",
        "    min=999999\n",
        "    for c in centroids:\n",
        "      newdist=abs(p-c)\n",
        "      if  newdist < min:\n",
        "        centroid=c\n",
        "        min = newdist\n",
        "    return [centroid,p]"
      ],
      "metadata": {
        "id": "qO5lzEWTUV-V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função K_Means - função principal\n",
        "  obtém a lista de centroids  randômicos - iniciais (centroids_updt)\n",
        "  inicializa a lista de centroids\n",
        "  realiza um loop até que o processo convirja e centroids == centroids_updt\n",
        "     a função map chama a findCluster-> forma o par (centroid, ponto)\n",
        "     utiliza a função groupByKey para gerar (K,[V])\n",
        "     calcula a media com a soma/|V| dos valores em V\n",
        "     transforma a tupla em  RDD com centroids\n",
        "     incrementa no acumulador o numero de iterações"
      ],
      "metadata": {
        "id": "tQJWHGbf04sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_means(k):\n",
        "\n",
        "  k_means = K_means_exemplo_MR(k)\n",
        "  centroids = list([])\n",
        "  centroids_updt = list(k_means.centroids)\n",
        "  print(\"centroids\"+str((centroids)))\n",
        "  print(\"centroid_upd\"+str(centroids_updt))\n",
        "  points=k_means.points\n",
        "  print(str(points.count()))\n",
        "  iter=0\n",
        "  while (sorted(centroids) != sorted(centroids_updt)):\n",
        "    centroids=centroids_updt\n",
        "    keysValues=points.map(lambda j:findCluster(j,centroids))\n",
        "    reducedKeys=keysValues.groupByKey().mapValues(lambda x: sum(x) / len(x))\n",
        "    newKeys=reducedKeys.map(lambda x:x[1])\n",
        "    centroids_updt=newKeys.collect()\n",
        "    acum.add(1)\n",
        "\n",
        "  return centroids_updt"
      ],
      "metadata": {
        "id": "M-w8-SyBSu6l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoca o process"
      ],
      "metadata": {
        "id": "M_uz2S9H21Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k=3\n",
        "groups = k_means(k)\n",
        "print(str(groups)+ \"Number of iterations: \"+str(acum))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGs3ek9liyqK",
        "outputId": "bad84ef9-d295-4b82-9daa-2a1f66881c68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centroids[]\n",
            "centroid_upd[21, 330, 519]\n",
            "1000\n",
            "[4072.2561307901906, 778.3677419354839, 2440.4210526315787]Number of iterations: 17\n"
          ]
        }
      ]
    }
  ]
}